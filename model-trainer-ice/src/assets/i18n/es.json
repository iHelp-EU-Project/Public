{
  "TypesAlgorithm.text" : "Tipos de algoritmos",
  "Myaccount.text" : "Mi cuenta",
  "Settings.text" : "Configuración",
  "SignOut.text" : "Cerrar sesión",
  "SignIn.text" : "Iniciar sesión",
  "Home.text" : "Principal",
  "NewsModel.text" : "Crear modelos",
  "MyModels.text" : "Mis modelos",
  "Search.text" : "Buscar",
  "Login.text" : "Iniciar sesión",
  "Password.text" : "Contraseña",
  "User.text": "Usuario",
  "Register.text": "Registrarse",
  "RequiredPass.text": "Contraseña requerida",
  "RequiredUser.text": "Nombre de usuario requerido",
  "unsupervised.text" : "Algoritmos No supervisados",
  "supervised.text": "Algoritmos supervisados",
  "Welcome.text": "Bienvenido",
  "errorUser1.text": "Error, usuario: ",
  "errorUser2.text": "no existe o contraseña incorrecta, Si no está registrado por favor regístrese.",
  "databaseOk.text": "Base de datos lista",
  "nodatabaseOk.text": "No se puede conectar con la base de datos aunque está lista. La razón es porque no está registrado en el sistema",
  "CreateContainer.text": "Creado el contenedor en el espacio de trabajo del modelo con éxito",
  "WorkspaceCreated.text": "Espacio de trabajo creado",
  "NoWorkspaceCreated.text": "Error, el contenedor en el espacio de trabajo del modelo no se ha creado, por favor, inténtelo de nuevo",
  "Required.text": "Obligatorio",
  "AlgoritmoMLColumns.text" : "Red Neuronal (valores en columnas)",
  "NameForMachineLearning.text" : "Nombre del modelo",
  "FileNameWithoutExtension.text" : "Nombre del modelo",
  "FileNameWithoutExtensionTitle.text" : "Nombre del modelo. No se permiten caracteres especiales ni espacios en blanco",
  "FileTraining.text": "Archivo de entrenamiento",
  "FileTrainingData.text": "Archivo de datos",
  "FileLabeling.text": "Archivo de etiquetas",
  "SelectCSVTrain.text": "Seleccione un archivo CSV de su sistema de archivos que se utilizará sólo para el entrenamiento (por ejemplo, machineData_trainData.csv)",
  "SelectCSVLabel.text": "Seleccione un archivo CSV de su sistema local que se utilizará para el etiquetado (por ejemplo, machineData_trainData.csv)",
  "TodownloadTrain.text" : "Para entrenar el modelo con un conjunto de datos de muestra",
  "TodownloadLabel.text" : "Para etiquetar el modelo con un conjunto de datos de muestra",
  "TodownloadTest.text" : "Para testear el modelo entrenado con un conjunto de datos de muestra",
  "FileTesting.text": "Archivo de testeo",
  "SelectCSVTest.text": "Seleccione un archivo CSV de su sistema de archivos que se utilizará sólo para el test (por ejemplo, machineData_testData.csv)",
  "Back.text": "Atrás",
  "Reset.text": "Reiniciar",
  "Next.text": "Siguiente",
  "RequiredTable.text" : "Obligatorio: Marque al menos una columna, por favor",
  "Rank.text" : "Rango",
  "ReconstructionMSE.text" : "Reconstrucción MSE (Error Cuadrático Medio)",
  "Model.text" : "Modelo: ",
  "Model2.text" : " creado y guardado en su espacio de trabajo: ",
  "FinishSave.text" : "Guardar y terminar",
  "FinishUploadModel.text" : "Guardar Modelo",
  "Train.text": "Entrenamiento",
  "Test.text": "Testeo",
  "TrainAnoma.text": "Distribución de anomalias (entrenamiento)",
  "TestAnoma.text": "Distribución de anomalias (testeo)",
  "Selectleast.text" : "Error, Seleccione al menos una columna de la tabla",
  "Correlation.text": "Correlación - ",
  "TableAnoTrain.text": "Tabla de anomalías (entrenamiento)",
  "TableAnoTest.text": "Tabla de anomalías (testeo)",
  "TablePredictionsColumns.text": "Predicciones (por columnas)",
  "TableAnoReconstructed.text": "Error reconstruido",
  "AlgoritmoMLColumnsID.text" : "Red Neuronal (valores con ID)",
  "CorrelationMatrixSelectedID.text" : "Matriz de correlación con ID",
  "Category.text": "Categoría",
  "Prediction.text" : "Predicción",
  "UserDelete.text": "Usuario eliminado",
  "UserError.text": "Error, usuario no eliminado, por favor, inténtelo de nuevo.",
  "WorkspaceDeleted.text": "Espacio de trabajo eliminado",
  "UserErrorUpdate.text": "Error, usuario no actualizado, por favor, inténtelo de nuevo.",
  "UserUpdated.text": "¡Usuario actualizado!, regístrese, por favor.",
  "UpdateSuccess.text": "Actualizar la cuenta con éxito",
  "UpdateAccount.text": "Tu cuenta",
  "ActualFirstName.text": "Nombre actual:",
  "ActualLastName.text": "Apellido(s) actual(es):",
  "NewPassword.text": "Nueva contraseña",
  "PassRequired.text":  "Se requiere una contraseña",
  "Update.text": "Actualizar",
  "DeleteAccount.text": "Borrar cuenta",
  "WarningDeleteAccount.text": "Si eliminas la cuenta, tanto los datos como el espacio de trabajo serán borrados y no se podrán recuperar",
  "FirstNameRequired.text": "El nombre es obligatorio",
  "LastNameRequired.text": "El(Los) apellido(s) es(son) obligatorio(s)",
  "Cancel.text": "Cancelar",
  "Read.text": "Leyendo modelo ",
  "Successful.text": " con éxito",
  "Error.text": "¡Error!",
  "NoReadSuccessful.text": " la lectura no tiene éxito, por favor, inténtelo de nuevo",
  "DownloadJSON.text": "Descargado el archivo JSON con éxito",
  "ErrorDonloadJSON.text": "Error, la descarga del archivo JSON no se ha realizado con éxito, por favor, inténtelo de nuevo",
  "Type.text": "Tipo:",
  "Models.text": "Modelos de: ",
  "ModelName.text": "Nombre del modelo: ",
  "Date.text": "Fecha: ",
  "ModelCategory.text": "Categoria del Modelo: ",
  "TypeAlgorithm.text": "Tipo de Algoritmo: ",
  "ModelID.text": "ID Modelo: ",
  "Regression.text": "Regresión",
  "ValuesID.text": "Valores con ID",
  "DescriptionGLM.text": "Los modelos lineales generalizados (GLM) estiman modelos de regresión. Para este caso el conjunto de datos almacena en una columna los valores de todos los sensores, y una segunda columna (ID) se utiliza para identificar la estación de trabajo y el sensor al que pertenece cada valor",
  "SupervisedAlgorithms.text" : "Algoritmos Supervisados",
  "NOSupervisedAlgorithms.text" : "Algoritmos No Supervisados",
  "DescriptionNN.text": "Esta versión del algoritmo de codificación de aprendizaje profundo (DLE) crea un modelo para la detección de anomalías basado en un grupo de columnas. Para este caso, el conjunto de datos debe por cada columna almacenar los valores de un solo sensor",
  "DescriptionNNID.text": "Esta versión del algoritmo de codificación de aprendizaje profundo (DLE) crea un modelo para la detección de anomalías basado en una columna específica. Para este caso, el conjunto de datos almacena en una columna los valores de todos los sensores, y una segunda columna (ID) se utiliza para identificar la estación de trabajo y el sensor al que pertenece cada valor",
  "DescriptionClustering.text": "El algoritmo K-Means clasifica los datos de entrada en conjuntos. Para este caso el conjunto de datos almacena en una columna los valores de todos los sensores, y una segunda columna (ID) se utiliza para identificar la estación de trabajo y el sensor al que pertenece cada valor",
  "NeuronalNetwork.text": "Red neuronal",
  "Clustering.text": "Algoritmos de agrupación",
  "ClusteringH20Algorithm.text": "Algoritmo de agrupación H2O",
  "ValuesPerColumns.text" : "Valores en columnas",
  "AllValues.text": "Todos los valores",
  "DescriptionPredator.text" : "Heurística Adaptativa para la Clasificación de Oponentes (AdHoc) es un algoritmo que permite a un agente crear clases de oponentes mientras interactúa con ellos. Se desarrolló para juegos multiagente iterativos (por ejemplo, el dilema del prisionero iterado) en los que el entorno no afecta a las decisiones del agente y es posible considerar encuentros individuales y aislados con los oponentes.",
  "spanish.text": "Español",
  "english.text": "Inglés",
  "Languages.text": "Seleccione su idioma predeterminado",
  "RegistrationSuccessful.text": "Registro realizado con éxito",
  "Registred.text": "Registrado, Gracias",
  "ErrorRegistred.text": "Error, el usuario no está registrado, por favor, inténtelo de nuevo",
  "WorkSpaceCreated.text": "Espacio de trabajo creado",
  "CreateWorkSpaceSuccessful": "Crear el espacio de trabajo del usuario con éxito",
  "ErrorWorkSpace.text": "Error, espacio de trabajo no creado, por favor, inténtelo de nuevo",
  "userName.text": "Usuario",
  "FirstName.text": "Nombre",
  "LastName.text": "Apellido(s)",
  "ClusteringChart.text": "Cuadro de agrupación",
  "TitleKmeans.text" : "Elija las propiedades para crear su modelo de predicción.  Tenga en cuenta que, dependiendo de lo que elija, tendrá un modelo de predicción mejor o peor.",
  "DescriptionKmeans.text" : "El agrupamiento es una forma de aprendizaje no supervisado que trata de encontrar patrones o grupos en los datos sin utilizar ninguna etiqueta o valor objetivo",
  "KmeansSplit.text" : "Partición del dataset: ",
  "KmeansSplitTitle.text":"Está diseñado para ser eficiente en big data utilizando un método de división probabilístico en lugar de una división exacta. Por ejemplo, al especificar una división de 0,75/0,25, el algoritmo producirá una división de prueba/entrenamiento con un valor esperado de 0,75/0,25 en lugar de exactamente 0,75/0,25",
  "KmeansSeed.text" : "Semilla:",
  "KmeansSeedTitle.text" : "Especifica la semilla del generador de números aleatorios (RNG). La semilla es consistente para cada instancia del algoritmo, de modo que se pueden crear modelos con las mismas condiciones iniciales en configuraciones alternativas.",
  "KmeansInit.text" : "Inicialización:",
  "KmeansInitTitle.text" : "Especifica el modo de inicialización.\n- La inicialización RANDOM muestrea aleatoriamente el valor especificado por k como centros de cluster en las filas de los datos de entrenamiento.\n- La inicialización PLUSPLUS elige al azar un centro inicial y pondera la selección aleatoria de los centros siguientes para que los puntos más alejados del primer centro tengan más probabilidades de ser elegidos.\n- La inicialización FURTHEST elige al azar un centro inicial y luego elige el siguiente centro para que sea el punto más alejado en términos de distancia euclidiana.\n- La inicialización USER requiere el correspondiente parámetro user_points. Nota: el conjunto de datos de puntos especificado por el usuario debe tener el mismo número de columnas que el conjunto de datos de entrenamiento.",
  "KmeansMaxIteration.text": "Iteraciones máximas:",
  "KmeansMaxIterationTitle.text": "Especifica el número máximo de iteraciones para el entrenamiento",
  "KmeansClusters.text": "Clusters:", 
  "KmeansClustersTitle.text": "Especifica el número de clusters (grupos de datos) de un conjunto de datos que son similares entre sí.",
  "KmeansStandarize.text": "Normalizar: ",
  "KmeansStandarizeTitle.text": "Active esta opción para normalizar las columnas numéricas para que tengan una media de cero y una varianza unitaria. Se recomienda encarecidamente la normalización; en caso que no se utilizara, los resultados pueden incluir componentes dominados por variables que parecen tener mayores varianzas en relación con otros atributos como una cuestión de escala, en lugar de una verdadera contribución",
  "KmeansClassColumn.text": "Columna de clase: ",
  "KmeansClassColumnTitle.text": "Columna que queremos predecir.",
  "Engagement.text":  "Compromiso",
  "DataMatrix.text": "Matriz de datos",
  "RegressionChart.text": "Tabla de regresión",
  "TotalWithinSS.text": "Total dentro de la SS",
  "BestModelPrediction.text": "Mejor cluster predicho",
  "Dispersion.text": "Dispersión del mejor cluster mostrando las columnas usadas para generar el modelo",
  "SettingsSave.text": "Configuración Guardada",
  "ChargeSettings.text": "Configuración cargada",
  "CorrelationMatrixKendall.text": "Matriz Kendall de correlación",
  "CorrelationMatrixPearson.text": "Matriz Pearson de correlación",
  "CorrelationMatrixSpearman.text": "Matriz Spearman de correlación",
  "CorrelationMatrix.text": "Matriz de correlación",
  "Goodbye.text": "Salida del sistema, adiós",
  "GLMFamily.text": "Familia:",
  "GLMFamilyTitle.text": "Las siguientes son las familias disponibles:\n- GAUSSIANA: la respuesta debe ser numérica (Real o Int). (por defecto).\n- BINOMIAL: la respuesta debe ser categórica de 2 niveles/clases o binaria (Enum o Int).\n- POISSON: la respuesta debe ser numérica y no negativa (Int).\n- GAMMA: la respuesta debe ser numérica, continua y positiva (Real o Int).\n- TWEEDIE: la respuesta debe ser numérica, continua (Real) y no negativa.\n- MULTINOMIAL: la respuesta puede ser categórica con más de dos niveles/clases (Enum).\n- ORDINAL: la respuesta debe ser categórica con al menos 3 niveles.\n- CUASIBINOMIAL: la respuesta debe ser numérica.\n- BINOMIAL NEGATIVA: la respuesta debe ser numérica y no negativa (Int)",
  "GLMSolver.text": "Soluciones:",
  "GLMSolverTitle.text": "Especifica el tipo de solucionador a utilizar (AUTO, IRLSM, L_BFGS, COORDINATE_DESCENT_NAÏVE, COORDINATE_DESCENT, GRADIENT_DESCENT_LH, o GRADIENT_DESCENT_SQERR):\n- IRLSM es rápido en problemas con un pequeño número de predictores y para la búsqueda lambda con penalización L1.\n- L_BFGS se adapta mejor a los conjuntos de datos con muchas columnas.\n- COORDINATE_DESCENT es IRLSM con la versión de actualizaciones de covarianza del descenso cíclico de coordenadas en el bucle más interno.\n- COORDINATE_DESCENT_NAÏVE es IRLSM con la versión de actualizaciones naïve del descenso cíclico de coordenadas en el bucle más interno.\n- GRADIENT_DESCENT_LH y GRADIENT_DESCENT_SQERR sólo pueden utilizarse con la familia ORDINAL.",
  "GLMLambda.text": "Lambda:",
  "GLMLambdaTitle.text": "Especifica el factor de regularización",
  "GLMMissing.text" : "Administración de valores no presentes: ",
  "GLMMissingTitle.text" : "Especifique cómo tratar los valores no presentes",
  "GLMTweedie.text" : "Tweedie:",
  "GLMTweedieTitle.text" : "(Sólo aplicable si se especifica Tweedie para Familia) Especifica el factor de la varianza en Tweedie",
  "GLMLink.text" : "Enlace:",
  "GLMLinkTitle.text" : "Tipos de enlace por familia:\n- GAUSSIANA, se admiten Identidad, Log e Inversa.\n- BINOMIAL, se admite Logit.\n- POISSON, se admite Log e Identidad.\n- GAMMA, se admite la Inversa, el Log y la Identidad.\n- TWEEDIE, sólo se admite Tweedie.\n- MULTINOMIAL, sólo se admite Family_Default.\n- QUASIBINOMIAL, sólo se admite Logit.\n- ORDINAL, sólo se admiten Ologit, Oprobit y Olog. (Tenga en cuenta que sólo Ologit está disponible para la regresión ordinal)\n- BINOMIAL NEGATIVA, sólo se admiten Log e Identidad.",
  "GLMIgnore.text": "Ignora:",
  "GLMIgnoreTitle.text": "Especifica si se activa la búsqueda de lambda, empezando por lambda max (lambda más pequeña que aproxima todos los coeficientes a cero)",
  "GLMAlpha.text": "Alpha:",
  "GLMAlphaTitle.text": "Especifica el máximo de iteraciones",
  "GLMLambdaSearch.text": "Búsqueda Lambda:",
  "GLMLambdaSearchTitle.text": "Especifica el número máximo de iteraciones en el entrenamiento",
  "GLMNfolds.text": "Iteraciones:",
  "GLMNfoldsTitle.text": "Especifique el número de iteraciones para la validación cruzada",
  "GLMKeeppredSearch.text": "Validación cruzada:",
  "GLMKeeppredTitle.text": "Especifica si se mantienen las predicciones de validación cruzada" ,
  "GLMColumnDepentVar.text": "Variable dependiente:",
  "GLMColumnDepentVarTitle.text": "Especifica la columna que se utilizará como variable dependiente.\n- Para un modelo de regresión, esta columna debe ser numérica (Real o Int).\n- Para un modelo de clasificación, esta columna debe ser categórica (Enum o String). Si la familia es Binomial, el conjunto de datos no puede contener más de dos niveles.",
  "PredictionGLM.text": "Predicción",
  "DispersionGLM.text": "Dispersión de las columnas utilizadas para generar el modelo",
  "Main1.text": "El aprendizaje automático (ML) puede ayudarle a utilizar los datos históricos para tomar mejores decisiones empresariales. Los algoritmos de ML descubren patrones en los datos y construyen modelos matemáticos a partir de estos descubrimientos. A continuación, se pueden utilizar los modelos para hacer predicciones sobre datos futuros.",
  "Main2.text": "Utilice el aprendizaje automático para las siguientes situaciones:",
  "Main3.text": "- No se pueden codificar las reglas: muchas tareas humanas (como reconocer si un correo electrónico es spam o no) no se pueden resolver adecuadamente con una solución simple (determinista) basada en reglas. Una gran cantidad de factores pueden influir en la respuesta. Cuando las reglas dependen de demasiados factores y muchas de estas reglas se superponen o necesitan ser ajustadas muy finamente, pronto se vuelve difícil para un humano codificar las reglas con precisión. Se puede usar ML para resolver este problema de manera efectiva.",
  "Main4.text": "- No se puede escalar: Es posible que pueda reconocer manualmente algunos cientos de correos electrónicos y decidir si son spam o no. Sin embargo, esta tarea se vuelve tediosa para millones de correos electrónicos. Las soluciones de ML son eficaces para tratar problemas a gran escala.",
  "Main5.text": "¿Qué es una fuente de datos?",
  "Main6.text": "El aprendizaje automático requiere que se aplique el conjunto correcto de datos a un proceso de aprendizaje para garantizar que se optimice la precisión de los modelos de aprendizaje automático; esta aplicación utiliza datos históricos almacenados en archivos con formato CSV.",
  "Main7.text": "¿En qué consiste el entrenamiento?",
  "Main8.text": "El proceso de entrenamiento de un modelo de ML implica proporcionar a un algoritmo de ML (es decir, el algoritmo de aprendizaje) datos de entrenamiento con los que aprender. El término modelo ML se refiere al artefacto del modelo que se crea mediante el proceso de entrenamiento.",
  "Main9.text": "Los datos de entrenamiento deben contener la respuesta correcta, que se conoce como objetivo o atributo objetivo. El algoritmo de aprendizaje encuentra patrones en los datos de entrenamiento que relacionan los atributos de los datos de entrada con el objetivo (la respuesta que se quiere predecir), y genera un modelo ML que captura estos patrones.",
  "Main10.text": "¿Qué es un modelo?",
  "Main11.text": "Un modelo de aprendizaje automático es el archivo de salida generado cuando se entrena el algoritmo de aprendizaje automático con datos históricos. Tras el entrenamiento, el modelo recibirá una entrada. Por ejemplo, un modelo de detección de anomalías puede consumir un flujo de datos para identificar anomalías en tiempo real basándose en los datos que entrenaron al modelo.",
  "PredatorRequirementsTitol.text": "Requisitos",
  "PredatorRequirements.text": " - Sube los archivos en grupos de 3 archivos mismo flujo de trabajo, perfil izquierdo (_1.csv), perfil derecho (_2.csv) y GPS (_gps.csv).",
  "PredatorRequirements2.text": "- Debemos añadir los archivos con este formato de nombre de archivo: `200117-002740_depot_gps.csv, 200117-002740_depot_1.csv, 200117-002740 _depot_2.csv`, sin espacios, y con los 13 primeros valores del nombre del fichero en este formato, por ejemplo `200117-002740_` y el último valor del nombre del fichero `_1`,` _2` y `_gps`.",
  "PredatorRequirementsLeft.text": "Perfil de la via izquierda.",
  "PredatorRequirementsRight.text":"Perfil de la via derecha.",
  "PredatorRequirementsGPS.text": "Perfil del GPS.",
  "PredatorRequirementsDataset.text": "Archivo de datos CSV o JSON",
  "PredatorRequirementsSensor.text": "Nombre del sensor",
  "PredatorRequirementsNumberRow.text": "Filas para generar modelos",
  "PredatorServerKafka.text": "Direccion Servidor Kafka",
  "PredatorSubmit.text": "Enviar",
  "GLMRegression.text": "Regresión GLM",
  "GLMDescription.text":  "Los modelos lineales generalizados (GLM) estiman modelos de regresión para resultados que siguen distribuciones exponenciales. Además de la distribución Gaussiana (es decir, normal), se incluyen las distribuciones Poisson, Binomial y Gamma. Cada una de ellas tiene un propósito diferente y, dependiendo de la distribución y la elección de la función de enlace, puede utilizarse para la predicción o la clasificación",
  "ModelDeleted.text": " eliminado, satifactoriamente ",
  "ModelDeletedError.text": " no se ha borrado, intentelo de nuevo.",
  "DeleteModel.text" : "Borrar modelo",
  "SaveModel.text": "Su modelo ha sido guardado en su espacio de trabajo",
  "WaitingPythonScripts.text" : "A la espera de datos, tenga paciencia",
  "EndingPythonScripts.text": "Terminando",
  "BuildingtraindataPythonScripts.text": "Construcción de datos de entrenamiento",
  "BuildingtestdataPythonScripts.text": "Construcción de datos de prueba",
  "StartingPythonScripts.text": "Empezando",
  "BuildingGraphicsPythonScripts.text" : "Construcción de los gráficos",
  "SavingGraphicsPythonScripts.text": "Guardando los gráficos",
  "BuildingCorrelationsPythonScripts.text":"Construir correlaciones",
  "PrintingCoorelationAnalysisPythonScripts.text" : "Escribiendo el analisis de las correlaciones",
  "SavingCoorelationsPythonScripts.text": "Guardar las coorrelaciones",
  "BuidingOjectCreateModelPythonScripts.text": "Construcción del objetco para crear un modelo",
  "BuidingModelPythonScripts.text": "Modelo en construcción",
  "BuidingMSEPythonScripts.text":"Construyendo el MSE",
  "BuidingModelsAnomaliesPythonScripts.text":"Construyendo los modelos para las anomalías",
  "RecoveringModelPythonScripts.text":"Recuperando el modelo",
  "BuidingPredictionsPythonScripts.text": "Construcción de predicciones",
  "OrderingModelPredictionsAnomaliesPythonScripts.text": "Ordenando el model, predicciones y anomalías",
  "SortingcoorelationsPythonScripts.text": "Clasificación de las coorrelaciones",
  "BuildingDatasetPythonScripts.text": "Construyendo un conjunto de datos",
  "BuildingDataChooseenUsersPythonScripts.text": "Construcción de datos elegidos por los usuarios",
  "BuildingDataRegressionPythonScripts.text" : "Construir regresión de datos",
  "DrawPlotsPythonScripts.text":"Dibujar gráficos",
  "ErrorFailsPythonScripts.text": "Error generico. Inténtelo de nuevo, gracias",
  "ErrorDataInputPythonScripts.text" : "Error en datos introducidos. Inténtelo de nuevo, gracias",
  "ErrorAlgorithmPythonScripts.text" : "Error en el algoritmo. Inténtelo de nuevo, gracias",
  "Recognition.text" : "Algoritmos Reconocimiento Imágenes",
  "CNNText.text" : "Redes neuronales convolucionales",
  "CNNs.text" : "(CNNs)",
  "RNNText.text" : "Redes neuronales recurrentes",
  "RNNs.text" : "(RNNs)",
  "RBMText.text" : "Máquinas de Boltzmann restringidas",
  "RBMs.text" : "(RBMs)",
  "AutoencodersText.text" : "Autoencoders",
  "DescriptionCNN.text" : "Las redes neuronales convolucionales son un tipo especial de red neuronal que imita a grandes rasgos la visión humana.",
  "DescriptionRNN.text" : "Una red neuronal recurrente es un tipo de red neuronal artificial que se utiliza habitualmente en el reconocimiento del habla y el procesamiento del lenguaje natural. Las redes neuronales recurrentes reconocen las características secuenciales de los datos y utilizan patrones para predecir el siguiente escenario probable.",
  "DescriptionRBN.text" : "Redes neuronales de avance con bucles de retroalimentación o retropropagación en el tiempo. Básicamente, son MLP (Perceptrón Multicapa) con una variable temporal adicional Máquinas",
  "DescriptionAutoencoder.text" : "Los autoencoders son redes neuronales con el objetivo de generar nuevos datos primero comprimiendo la entrada en un espacio de variables latentes y luego reconstruyendo la salida en base a la información adquirida",
  "PicturesTraining.text": "Imágenes de entrenamiento",
  "FirstLayerConvolution.text": "Primera capa de convolucion por imágen para entrenar el modelo en la red neuronal",
  "SecondLayerConvolution.text": "Última capa de convolución por conjunto de imágenes del modelo de la red neuronal",
  "SingleTestCNN.text": "Test simple para una imágen aleatoria de las imágenes de validación con el modelo recien contruido",
  "GroupTestCNN.text": "Test con un grupo de imagenes de validación guardadas para comprobar la validez del modelo",
  "EntropyCNN.text": "Pérdida de entropía cruzada y precisión de la clasificación.",
  "RedTestCNN.text": "Texto en rojo imagen mal identificada",
  "TitleTestModelCNN.text": "Validacion del Modelo: ",
  "DragANDDropCNN.text": "Arrastre y suelte el archivo aquí y pruebe su modelo",
  "BrowseFilesCNN.text": "Buscar archivos",
  "NumberImages.text": "Número de clases o tipo de imágenes: (máx.10)",
  "ListImagesMenu.text" : "Lista de imágenes",
  "LoadImagesUI.text": "Cargar Imagenes",
  "GenerareClass.text": "Generar clases",
  "BatchNormalization.text": "Normalización por lotes",
  "layerConvolution.text": "Número de capas de convolución en la red neuronal.",
  "Dropout.text": "% de perdida de las capa: ",
  "Dense.text": "Densidad neuronal en capa: ",
  "layer.text": "Número las capas de la red neuronal.",
  "epochs.text": "Número de épocas en la red neuronal",
  "numberNeuros.text": "Número de neuronas en la red neuronal.",
  "activation.text": "Activación",
  "TiltleAction.text": "Una función de activación en una red neuronal define cómo la suma ponderada de la entrada se \n transforma en una salida de un nodo o nodos en una capa de la red",
  "relu.text": "Activación lineal rectificada (ReLU)",
  "sigmoid.text": "Logística (Sigmoide)",
  "tanh.text": "Tangente hiperbólica (Tanh)",
  "softmax.text": "Función exponencial normalizada (softmax)",
  "InserClasses1.text": " Las imágenes mínimas para este modelo son ",
  "InserClasses2.text": ", imágenes actuales: ",
  "Accuracy.text": " Precisión ",
  "Loss.text": " Pérdida ",
  "InformationCNN.text": "Si quieres probar el modelo búscalo en la sección 'Mymodels'.",
  "cnn.text": "Modelos CNN",
  "glm.text": "Modelos GLM",
  "id.text": "Modelos por ID",
  "kmeans.text": "Modelos Clustering",
  "columns.text": "Modelos por  Columns",
  "all.text": "Todos los modelos",
  "SearchPerModel.text": " Filtrado por modelo. ",
  "Nlayer.text": " capas",
  "SearchPerName.text": " Search per Name. ",
  "NotFound.text": " Not Found ",
  "featurewiseCenter.text" : "Centro de características",
  "classMode.text": "Modo de clase",
  "classModeTitle.text": "Set 'binary' if you have only two classes to predict, if not set to 'categorical', in case if you're developing an Autoencoder system, both input and the output would probably be the same image, for this case set to 'input'",
  "colorMode.text": "Modo de color",
  "colorModeTitle.text": "Si la imagen es en blanco y negro o en escala de grises establezca 'grayscale' o si la imagen tiene tres canales de color, establezca 'rgb'.",
  "shuffle.text": "Barajar Orden",
  "shuffleTitle.text": "Establezca True si desea barajar el orden de la imagen que se está produciendo, de lo contrario establezca False.",
  "seed.text": "Semillas",
  "seedTitle.text": "Semilla aleatoria para aplicar el aumento aleatorio de la imagen y barajar el orden de la misma.",
  "batchSize.text": "Batch Size",
  "batchSizeTitle.text": "Number of images to be yielded from the generator per batch",
  "imgHeight.text": "Resize per Height",
  "imgHeightTitle.text": "is the size of your input images, every image will be resized to this size, per height",
  "imgWidth.text": "Resize per Width",
  "imgWidthTitle.text": "is the size of your input images, every image will be resized to this size, per width",
  "learningRate.text": "Tasa de aprendizaje",
  "learningRateTitle.text": "La tasa de aprendizaje. El valor predeterminado es 0,01. ",
  "momentum.text": "Impulso",
  "momentumTitle.text": "Hiperparámetro flotante >= 0 que acelera el descenso de gradiente en la dirección relevante y amortigua las oscilaciones. Por defecto es 0, es decir, descenso de gradiente suave.  ",
  "lossCompile.text": "Función de pérdida",
  "lossCompileTitle.text": "Puede ser una cadena (nombre de la función de pérdida), o una instancia de tf.keras.losses.Loss. Ver tf.keras.losses. Una función de pérdida es cualquier llamada con la firma loss = fn(y_true, y_pred), donde y_true son los valores de la verdad y y_pred son las predicciones del modelo. y_true debe tener la forma (batch_size, d0, .. dN) (excepto en el caso de las funciones de pérdida dispersas, como la entropía cruzada categórica dispersa, que espera matrices enteras de forma (batch_size, d0, .. dN-1)). y_pred debe tener forma (batch_size, d0, .. dN). La función de pérdida debe devolver un tensor de flotación. Si se utiliza una instancia de pérdida personalizada y la reducción se establece en None, el valor de retorno tiene forma (batch_size, d0, .. dN-1), es decir, valores de pérdida por muestra o por tiempo; de lo contrario, es un escalar. Si el modelo tiene múltiples salidas, puede utilizar una pérdida diferente en cada salida pasando un diccionario o una lista de pérdidas. El valor de pérdida que será minimizado por el modelo será entonces la suma de todas las pérdidas individuales, a menos que se especifique loss_weights. ",
  "sparse_categorical_crossentropyTitle.text": "Calcula la pérdida de entropía cruzada entre las etiquetas y las predicciones.",
  "binary_crossentropyTitle.text": "Calcula la pérdida de entropía cruzada entre las etiquetas verdaderas y las predichas.",
  "categorical_crossentropyTitle.text": "Calcula la pérdida de entropía cruzada entre las etiquetas y las predicciones.",
  "categorical_hingeTitle.text": "Calcula la pérdida de categoría de la caducidad entre y_true y y_pred.",
  "cosine_similarityTitle.text": "Calcula la similitud del coseno entre las etiquetas y las predicciones.",
  "hingeTitle.text": "Calcula la pérdida de acoplamiento entre y_true y y_pred.",
  "huberTitle.text": "Calcula la pérdida de Huber entre y_true y y_pred.",
  "kldivergenceTitle.text": "Calcula la pérdida de divergencia de Kullback-Leibler entre y_true y y_pred.",
  "log_coshTitle.text": "Calcula el logaritmo del coseno hiperbólico del error de predicción.",
  "lossTitle.text": "Clase base de pérdida.",
  "mean_absolute_errorTitle.text": "Calcula la media de la diferencia absoluta entre las etiquetas y las predicciones.",
  "mean_absolute_percentage_errorTitle.text": "Calcula el error medio porcentual absoluto entre y_true y y_pred.",
  "mean_squared_errorTitle.text": "Calcula la media de los cuadrados de los errores entre las etiquetas y las predicciones.",
  "mean_squared_logarithmic_errorTitle.text": "Calcula el error logarítmico medio al cuadrado entre y_true y y_pred.",
  "poissonTitle.text": "Calcula la pérdida de Poisson entre y_true y y_pred.",
  "reductionTitle.text": "Tipos de reducción de pérdidas.",
  "squaredhingeTitle.text": "Calcula la pérdida de acoplamiento al cuadrado entre y_true y y_pred.",
  "kernelInitializer.text": "Inicializador del núcleo",
  "kernelInitializerTitle.text": "Los inicializadores definen la forma de establecer los pesos aleatorios iniciales de las capas de Keras. La palabra clave de argumentos utilizada para pasar los inicializadores a las capas depende de la capa. Normalmente, es simplemente kernel_initializer y bias_initializer : from tensorflow.keras import layers from tensorflow.keras import initializers layer = layers.Los inicializadores definen la forma de establecer los pesos aleatorios iniciales de las capas de Keras. La palabra clave de argumentos utilizada para pasar los inicializadores a las capas depende de la capa. ",
  "ConstantTitle.text": "Inicializador que genera tensores con valores constantes.",
  "GlorotNormalTitle.text": "El inicializador normal Glorot, también llamado inicializador normal Xavier.",
  "GlorotUniformTitle.text": "El inicializador uniforme Glorot, también llamado inicializador uniforme Xavier.",
  "HeNormalTitle.text": "El inicializador normal.",
  "HeUniformTitle.text": "El inicializador de escala de varianza uniforme.",
  "IdentityTitle.text": "Inicializador que genera la matriz de identidad.",
  "InitializerTitle.text": "Inicializador base : todos los inicializadores de Keras heredan de este .",
  "LecunNormalTitle.text": "Inicializador normal de Lecun.",
  "LecunUniformTitle.text": "Inicializador uniforme de Lecun.",
  "OnesTitle.text": "Inicializador que genera tensores inicializados a 1.",
  "OrthogonalTitle.text": "Inicializador que genera una matriz ortogonal.",
  "RandomNormalTitle.text": "Inicializador que genera tensores con una distribución normal.",
  "RandomUniformTitle.text": "Inicializador que genera tensores con una distribución uniforme.",
  "TruncatedNormalTitle.text":" Inicializador que genera una distribución normal truncada.",
  "VarianceScalingTitle.text":" Inicializador capaz de adaptar su escala a la forma de los tensores de pesos.",
  "ZerosTitle.text":" Inicializador que genera tensores inicializados a 0.",
  "typeImages.text": "Tipos de ímagenes",
  "ClusteringH2O.text": "Agrupación mediante el algoritmo Kmeans H2O",
  "ClusteringTensor.text": "Agrupación mediante el algoritmo TensorFlow",
  "ClusteringWard.text": "Agrupación mediante el algoritmo de Ward",
  "ClusteringWardAlgorithm.text": "Algoritmo de agrupación Ward",
  "ClusteringBirchAlgorithm.text": "Algoritmo de agrupación Birch",
  "ClusteringBirch.text": "Agrupación mediante el algoritmo de Birch",
  "ClusteringSklearn.text": "Agrupación mediante el algoritmo Kmeans Sklearn",
  "ClusteringSklearnAlgorithm.text": "Algoritmo de clustering Sklearn",
  "SklearnClusteringRandomState.text": "Estado aleatorio: ",
  "SklearnClusteringRandomStateTitle.text": "Determina la generación de números aleatorios para la inicialización del centroide. Utilice un int para que la aleatoriedad sea determinista.",
  "SklearnnInit.text": "Número de entrada:",
  "SklearnnInitTitle.text": "Número de veces que se ejecutará el algoritmo k-means con diferentes semillas de centroides. El resultado final será el mejor resultado de n_init ejecuciones consecutivas en términos de inercia.",
  "WaterFall.Text" : "Este gráfico nos ayuda a visualizar los valores SHAP de cada una de las características. Nos indican en qué medida cada una de las características ha aumentado o disminuido el número de columnas previsto para esta columna específica.",
  "MeanBar.Text": "Otra forma de agregar los valores es utilizar un gráfico SHAP medio. Para cada característica, calculamos la media de los valores SHAP absolutos de todas las observaciones. Tomamos los valores absolutos porque no queremos que los valores positivos y negativos se compensen entre sí. Hay una barra para cada característica y podemos ver que el peso de la cáscara tenía la mayor media SHAP de todas las características.",
  "SummaryPlotBarViolin.Text": "El gráfico de resumen SHAP puede mostrar las relaciones positivas y negativas de los predictores con la variable objetivo, en gráfico de violín",
  "SummaryPlotBarDot.Text": "Este es un gráfico de todos los valores SHAP. Los valores están agrupados por las características en el eje Y. En cada grupo, el color de los puntos viene determinado por el valor de la misma característica (es decir, los valores más altos de la característica son más rojos). Las características se ordenan por los valores medios de SHAP. ",
  "DecisionPlot.Text": "Ya podemos ver algunas tendencias. Por ejemplo, algunas de las líneas parecen zigzaguear en la parte superior del gráfico. Para estas observaciones, el peso de la algunas columnas aumenta la predicción (es decir, SHAP positivo) y el peso de la cáscara y el peso entero disminuyen la predicción (es decir, SHAP negativo). En otras palabras, estas características tienen efectos opuestos en la predicción",
  "HeatMap.Text" : "Un mapa de calor visualiza la magnitud de los datos en dos dimensiones. La variación de color puede dar pistas visuales sobre cómo se agrupan los datos. Se puede utilizar el esquema de colores de caliente a frío para mostrar las relaciones. La importancia de la variable se muestra en orden descendente en el eje Y, con las barras parecidas a las del gráfico de barras. La curva f(x) en la parte superior es la predicción del modelo para las instancias. El SHAP ejecuta primero un clustering jerárquico en las instancias para agruparlas, y luego ordena las instancias en el eje X. El centro del mapa de calor 2D es el valor_base, que es la predicción media para todas las instancias",
  "ShapExplanation.Text" : "Explicación del modelo construido por el método SHAP",
  "DataPlotEx.Text": "Conjunto de datos original",
  "Weight.Texto": "Podemos centrarnos en estas relaciones utilizando los gráficos de dispersión. Aquí podemos ver claramente cómo la relación entre los valores SHAP y los valores de las características es positiva y negativa respectivamente",
  "Legend.Text": "Unos datos puede dividirse en dos o más grupos según los predictores. Esto proporciona más información sobre la heterogeneidad de los datos. Puede utilizar el cohorts(N) para dividir la población en N cohortes.",
  "Observation.Texto": "Cada valor SHAP.\n El eje x representa la salida del modelo. En este caso, las unidades son las probabilidades logarítmicas. El gráfico se centra en el eje x en explainer.expected_value. Todos los valores SHAP son relativos al valor esperado del modelo, como los efectos de un modelo lineal son relativos al intercepto.\n El eje Y muestra las características del modelo. Por defecto, las características se ordenan por importancia descendente. La importancia se calcula sobre las observaciones trazadas. Esto suele ser diferente a la ordenación de la importancia para todo el conjunto de datos. Además de la ordenación de la importancia de las características, el gráfico de decisión también admite la ordenación jerárquica de las características de los clusters y la ordenación de las características definida por el usuario.\n La predicción de cada observación se representa con una línea de color. En la parte superior del gráfico, cada línea se sitúa en el eje x en el valor de predicción de la observación correspondiente. Este valor determina el color de la línea en un espectro.\nDesplazándose desde la parte inferior del gráfico hacia la parte superior, los valores SHAP de cada característica se añaden al valor base del modelo. Esto muestra cómo cada característica contribuye a la predicción global.\n En la parte inferior del gráfico",
  "DecisionPlotAll.Text": "Todos los valores SHAP, pero con link modo 'logic'.\n El eje x representa la salida del modelo. En este caso, las unidades son las probabilidades logarítmicas. El gráfico se centra en el eje x en explainer.expected_value. Todos los valores SHAP son relativos al valor esperado del modelo, como los efectos de un modelo lineal son relativos al intercepto.\n El eje Y muestra las características del modelo. Por defecto, las características se ordenan por importancia descendente. La importancia se calcula sobre las observaciones trazadas. Esto suele ser diferente a la ordenación de la importancia para todo el conjunto de datos. Además de la ordenación de la importancia de las características, el gráfico de decisión también admite la ordenación jerárquica de las características de los clusters y la ordenación de las características definida por el usuario.\n La predicción de cada observación se representa con una línea de color. En la parte superior del gráfico, cada línea se sitúa en el eje x en el valor de predicción de la observación correspondiente. Este valor determina el color de la línea en un espectro.\nDesplazándose desde la parte inferior del gráfico hacia la parte superior, los valores SHAP de cada característica se añaden al valor base del modelo. Esto muestra cómo cada característica contribuye a la predicción global.\n En la parte inferior del gráfico," ,
  "DecisionPlotAllN.Text": "Todos los valores SHAP, pero con link 'normal'.\n El eje x representa la salida del modelo. En este caso, las unidades son las probabilidades logarítmicas. El gráfico se centra en el eje x en explainer.expected_value. Todos los valores SHAP son relativos al valor esperado del modelo, como los efectos de un modelo lineal son relativos al intercepto.\n El eje Y muestra las características del modelo. Por defecto, las características se ordenan por importancia descendente. La importancia se calcula sobre las observaciones trazadas. Esto suele ser diferente a la ordenación de la importancia para todo el conjunto de datos. Además de la ordenación de la importancia de las características, el gráfico de decisión también admite la ordenación jerárquica de las características de los clusters y la ordenación de las características definida por el usuario.\n La predicción de cada observación se representa con una línea de color. En la parte superior del gráfico, cada línea se sitúa en el eje x en el valor de predicción de la observación correspondiente. Este valor determina el color de la línea en un espectro.\nDesplazándose desde la parte inferior del gráfico hacia la parte superior, los valores SHAP de cada característica se añaden al valor base del modelo. Esto muestra cómo cada característica contribuye a la predicción global.\n En la parte inferior del gráfico",
  "BarForce.Text": "Otra forma de visualizar los valores de SHAP es mediante un gráfico de fuerzas. Estos nos dan prácticamente la misma información que un gráfico de cascada. Aqui, podemos ver que empezamos con el mismo valor base. También puede ver cómo cada característica aumenta/disminuye el valor predicho para darnos la predicción final.",
  "Multiclass.Texto": "Puede utilizar el gráfico de resumen para mostrar la importancia de la variable por clase",
  "ShapMulticlass.Text": "Puede haber construido un modelo multiclase que clasifique las instancias en varias clases. \n La salida de un modelo multiclase es una matriz de probabilidad para las clases. Si tenemos tres clases, las salidas son las probabilidades de las tres clases, que suman 1.0. Las probabilidades se muestran en las columnas de las clases. La función .predict() muestra la clasificación predicha, como se muestra en la columna 'Pred'. ",
  "SklearnClusteringTBirchhresholdTitle.Text": "El radio del subgrupo obtenido al fusionar una nueva muestra y el subgrupo más cercano debe ser menor que el umbral. \n De lo contrario, se inicia un nuevo subgrupo. Si este valor es muy bajo, se promueve la división y viceversa.",
  "SklearnClusteringTBirchhresholdState.Text": "Umbral: ",
  "SklearnBirchBranchingFactorTitle.Text": "Número máximo de subgrupos de CF en cada nodo. Si entra una nueva muestra tal que el número de subgrupos supera el factor de ramificación, entonces ese nodo se divide en dos nodos con los subgrupos redistribuidos en cada uno. \n - Se elimina el subcluster padre de ese nodo y se añaden dos nuevos subclusters como padres de los 2 nodos divididos.",
  "SklearnBirchBranchingFactor.Text": "Factor de ramificación: ",
  "SklearnBirchComputeLabelsTitle.Text": "Si se calculan o no las etiquetas para cada ajuste.",
  "SklearnBirchComputeLabels.Text": "Calcular las etiquetas: ",
  "SklearnClusteringTWardaffinityTitle.Text": "Métrica utilizada para calcular el vínculo.",
  "SklearnClusteringTWardaffinityState.Text": "Afinidad: ",
  "SklearnWardcomputeFullTreeTitle.Text": "Detener anticipadamente la construcción del árbol en n_clusters. Esto es útil para disminuir el tiempo de cálculo si el número de clusters no es pequeño comparado con el número de muestras.  \n Esta opción sólo es útil cuando se especifica una matriz de conectividad. Tenga en cuenta también que cuando se varía el número de conglomerados y se utiliza \n  la caché, puede ser ventajoso calcular el árbol completo. Debe ser True si distancia_umbral no es None. Por defecto computar árbol completo es auto, que equivale a True cuando  \n  distancia umbral no es None o que n_clusters es inferior al máximo entre 100 o 0,02 * n_muestras. En caso contrario, auto equivale a False.",
  "SklearnWardcomputeFullTree.Text": "Calcular árbol completo: ",
  "SklearnWardcomputeDistancesTitle.Text": "Calcula las distancias entre los clusters aunque no se utilice el umbral de distancia. Esto se puede utilizar para hacer la visualización del dendrograma, pero introduce una sobrecarga computacional y de memoria.",
  "SklearnWardcomputeDistances.Text": "Calcular distancias: ",
  "CleanDataPythonScripts.Text": "Clearing data ",
  "GenerateShapPlotsDataPythonScripts.Text": "Generación de gráficos y datos de Shap",
  "BuildingKMEANSAlgorithmPythonScripts.Text": "Construcción de la SSE de clustering K-MEANS ",
  "ClustersSilhouettePythonScripts.Text": "Construyendo el numero de cluster frente a la puntuación de la silueta",
  "BuildingModelBirchPythonScripts.Text": "Generando modelo Birch ",
  "BuildingRelativeImportanceAttributesPythonScripts.Text": "Generar la importancia relativa de los atributos ",
  "BuildingRelativeImportanceHeatmapPythonScripts.Text": "Construyendo el mapa de calor",
  "SaveModelJSONPythonScripts.Text": "Guatdando el modelo y el archivo JSON",
  "CompletingFinalOperationsPythonScripts.Text": "Completar las operaciones finales ",
  "BuildingModelSingle.Text": "Construyendo el modelo Single ",
  "BuildingModelComplete.Text": "Construyendo el modelo Complete ",
  "BuildingModelWard.Text": "Construyendo el modelo Ward ",
  "BuildingModelAverage.Text": "Construyendo el modelo Average ",
  "SeeDistorsions.Text": "SEE y Distorsiones por Atributos del Clúster",
  "PredictionLabel.Text": "Predicción por clúster",
  "TrueLabel.Text": "Datos Originales",
  "BIRCHBarStandardizedAttributes.Text": "BIRCH: Diagrama de barras de los atributos estandarizados",
  "BIRCHSnakeStandardizedAttributes.Text": "BIRCH: Gráfico de la serpiente de atributos estandarizados",
  "BIRCHRelativeImportanceAttributes.Text": "BIRCH: Importancia relativa de los atributos" ,
  "AllColumnsForTrainModel.Text": "Seleccionar todas las columnas",
  "DeleteAllColumns.text": "Seleccionar para borrar todas las columnas",
  "ClustingSingleLink.Text": "Agrupación por enlace único",
  "ClustingCompleteLink.Text": "Agrupación por enlace completo",
  "ClustingWardLink.Text": "Agrupación por Ward Link",
  "ClustingAverageLink.Text": "Agrupación por enlace medio",
  "SnakePlot.Text":"Gráfico de Serpiente",
  "BarPlot.Text": "Gráfico de Barras",
  "LinkName.Text": "Enlace: ",
  "linkTitle.Text": "Qué criterio de vinculación . El criterio de vinculación determina la distancia a utilizar entre los conjuntos de observación. \n El algoritmo fusionará los pares de cluster que minimicen este criterio. \n 'ward' minimiza la varianza de los clusters que se fusionan. \n 'average' utiliza la media de las distancias de cada observación de los dos conjuntos. \n 'complete' o 'máxima' vinculación utiliza las distancias máximas entre todas las observaciones de los dos conjuntos. \n 'single' utiliza el mínimo de las distancias entre todas las observaciones de los dos conjuntos.",
  "ValuesNan.Text": "Valores NULL y NAN",
  "ValuesNanTitle.Text": "Esta parte es sólo para saber cómo quiere manejar los valores NULL y NAN de los conjuntos de datos: \n - Drop: Elimina las filas con valores NULL y NaN. \n - With0: Rellena los valores NULL y NaN con ceros. \n - Fill: Propagar valores no nulos hacia adelante o hacia atrás. \n - Quartile min: Rellena los valores NULL y NaN de la columna con el cuartil más bajo de los valores de la columna. \n - Quartile 25%:  Rellena los valores NULL y NaN de la columna con el cuartil 25% de los valores de la columna. \n - Quartile 75%:  Rellena los valores NULL y NaN de la columna con el cuartil 75% de los valores de la columna. \n - Quartile Max: Rellena los valores NULL y NaN de la columna con el cuartil máximo de los valores de la columna. \n - Mediana:  Rellena los valores NULL y NaN de la columna con la mediana de los valores de la columna. \n - Media:  Rellena los valores NULL y NaN de la columna con la media de los valores de la columna. \n - Mode: Sustituir por el Valor más repetido en la columna ", 
  "Authorization.Text": "Lo sentimos, pero no tiene permiso para utilizar este algoritmo, por favor, póngase en contacto con el administrador.",
  "Authentication.Text": "Lo sentimos, pero no está autentificado en el sistema, por favor, póngase en contacto con el administrador.",
  "DescriptionGAN.text" : "Las redes generativas adversariales (GAN) son arquitecturas algorítmicas que utilizan dos redes neuronales, enfrentando una contra otra (de ahí lo de 'adversarial') con el fin de generar nuevas instancias sintéticas de datos que puedan pasar por datos reales. Se utilizan mucho en la generación de imágenes, vídeo y voz.",
  "GANText.Text" : "GAN X Ray" ,
  "GANs.Text" : "(GAN)",
  "AdvancedModelTitle.Text": "Modelo se ha entrenado sobre datos primarios solo o datos primarios + secundarios",
  "SklearnBirchAdvancedModel.Text" : "Modelos avanzados",
  "Descriptionward.text" : "El algoritmo de clustering Ward en scikit-learn es una técnica que agrupa datos de manera jerárquica para minimizar la variabilidad dentro de cada grupo.\n Es útil para encontrar grupos de datos similares y se basa en fusionar gradualmente clusters cercanos entre sí. Este enfoque es comúnmente utilizado en análisis de datos y segmentación.",
  "DescriptionBirch.text" : "BIRCH es un algoritmo de clustering en scikit-learn que se destaca por su capacidad para manejar grandes conjuntos de datos mediante la construcción de un árbol BIRCH, lo que lo hace eficiente y escalable para aplicaciones de agrupamiento.",
  "DescriptionskClust.text" : "K-Means es un algoritmo de clustering que agrupa datos en K clusters, donde K es un número que se define previamente. El objetivo principal es asignar cada punto de datos al cluster más cercano según su distancia a los centroides del cluster.\n Scikit-learn proporciona una implementación flexible y fácil de usar de K-Means que permite especificar el número de clusters y ajustar el modelo a tus datos."



  












}