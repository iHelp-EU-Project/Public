{
  "TypesAlgorithm.text" : "Types of algorithms",
  "Myaccount.text" : "My Account",
  "Settings.text" : "Settings",
  "SignOut.text" : "Log Out",
  "SignIn.text" : "Log In",
  "Home.text" : "     Home",
  "NewsModel.text" : "New Model",
  "MyModels.text" : "My Models",
  "Search.text" : "Search",
  "Login.text" : "Login",
  "Password.text" : "Password",
  "User.text": "Username",
  "Register.text": "Register",
  "RequiredPass.text": "Password is required",
  "RequiredUser.text": "Username is required",
  "unsupervised.text" : "Unsupervised algorithms",
  "supervised.text": "Supervised algorithm",
  "Welcome.text": "Welcome",
  "errorUser1.text": "Error! User: ",
  "errorUser2.text": "It either doesn't exist or a bad password has been provided. If you are not registered, please register",
  "databaseOk.text": "Database is up & running",
  "nodatabaseOk.text": "Cannot connect to database although it is accessible. Reason is that user is not logged in the system",
  "CreateContainer.text": "Container successfully created in the model workspace",
  "WorkspaceCreated.text": "Workspace created",
  "NoWorkspaceCreated.text": "Error! Failed to create the container in the model workspace. Please, try again",
  "Required.text": "Required",
  "AlgoritmoMLColumns.text" : "Neural Network (values in columns)",
  "NameForMachineLearning.text" : "Model name",
  "FileNameWithoutExtension.text" : "Model name",
  "FileNameWithoutExtensionTitle.text" : "Model name. Special characters or blank spaces are not allowed",
  "FileTraining.text": "Training file",
  "FileTrainingData.text": "Data file",
  "FileLabeling.text": "Label file",
  "SelectCSVTrain.text": "Select a CSV file from your local system to be used for training (e.g. machineData_trainData.csv)",  
  "SelectCSVLabel.text": "Select a CSV file from your local system to be used for labeling (e.g. machineData_trainData.csv)",
  "TodownloadTrain.text" : "To train the model with a sample dataset",
  "TodownloadLabel.text" : "To label the model with a sample dataset",
  "TodownloadTest.text" : "To test the trained model with a sample testing dataset",
  "FileTesting.text": "Testing file",
  "SelectCSVTest.text": "Select a CSV file from your local system to be used for testing (e.g. machineData_testData.csv)",
  "Back.text": "Back",
  "Reset.text": "Reset",
  "Next.text": "Next",
  "RequiredTable.text" : "Required: please, check at least one column",
  "Rank.text" : "Range",
  "ReconstructionMSE.text" : "Reconstructed MSE (Mean Squared Error)",
  "Model.text" : "Model: ",
  "Model2.text" : " created and saved in your workspace: ",
  "FinishSave.text" : "Save & Finish",
  "FinishUploadModel.text" : "Save Model",
  "Train.text": "Training",
  "Test.text": "Testing",
  "TrainAnoma.text": "Anomalies distribution (training)",
  "TestAnoma.text": "Anomalies distribution (testing)",
  "Selectleast.text" : "Error! Select at least one column of the table",
  "Correlation.text": "Correlation - ",
  "TableAnoTrain.text": "Anomalies table (trained)",
  "TableAnoTest.text": "Anomalies table (tested)",
  "TablePredictionsColumns.text": "Predictions (by columns)",
  "TableAnoReconstructed.text": "Reconstructed error",
  "AlgoritmoMLColumnsID.text" : "Neural Network (values with ID)",
  "CorrelationMatrixSelectedID.text" : "Correlation Matrix Selected ID",
  "Category.text": "Category",
  "Prediction.text" : "Prediction",
  "UserDelete.text": "User deleted",
  "UserError.text": "Error! User couldn't be deleted. Please, try again",
  "WorkspaceDeleted.text": "Workspace deleted",
  "UserErrorUpdate.text": "Error! User couldn't be updated. Please, try again",
  "UserUpdated.text": "User updated!, sign in, please.",
  "UpdateSuccess.text": "Update Account successful",
  "UpdateAccount.text": "Your Account",
  "ActualFirstName.text": "Current first name:",
  "ActualLastName.text": "Current last name:",
  "NewPassword.text": "New Password",
  "PassRequired.text":  "Password is required",
  "Update.text": "Update",
  "DeleteAccount.text": "Delete Account",
  "WarningDeleteAccount.text": "If you delete the account, everything will be deleted and data and workspaces cannot be recovered",
  "FirstNameRequired.text": "First name is required",
  "LastNameRequired.text": "Last name is required",
  "Cancel.text": "Cancel",
  "Read.text": "Read ",
  "Successful.text": " successfully",
  "Error.text": "Error! ",
  "NoReadSuccessful.text": "81. reading failed. Please, try again",
  "DownloadJSON.text": "JSON file successfully downloaded",
  "ErrorDonloadJSON.text": "Error! JSON file couldn't be downloaded. Please, try again",
  "Type.text": "Type:",
  "Models.text": "Models from: ",
  "ModelName.text": "Model name: ",
  "Date.text": "Date: ",
  "ModelCategory.text": "Model Category: ",
  "TypeAlgorithm.text": "Type of Algorithm: ",
  "ModelID.text": "Model ID: ",
  "Regression.text": "Regression",
  "ValuesID.text": "Values with ID",
  "DescriptionGLM.text": "Generalized Linear Models (GLM) estimate regression models. For this case, the dataset store in one column the values of all sensors, while a second column (ID) is used to identify workstation and sensor to which each value belongs to",
  "SupervisedAlgorithms.text" : "Supervised Algorithms",
  "NOSupervisedAlgorithms.text" : "Unsupervised Algorithms",
  "DescriptionNN.text": "This version of Deep Learning Encoder Algorithm (DLE) creates a model for anomaly detection based on a group of columns. For this case, the dataset stores for each column the values of only one sensor",
  "DescriptionNNID.text": "This version of Deep Learning Encoder Algorithm (DLE) creates a model for anomaly detection based on a specific column. For this case, the dataset stores in one column the values of all sensors, while a second column (ID) is used to identify the workstation and sensor to which each value belongs to",
  "DescriptionClustering.text": "The K-Means algorithm classifies the input data in a set of clusters. For this case, the dataset stores in one column the values of all sensors, while a second column (ID) is used to identify the workstation and sensor to which each value belongs to",
  "NeuronalNetwork.text": "Neural Network",
  "Clustering.text": "Clustering algorithms",
  "ClusteringH20Algorithm.text": "Clustering algorithm H2O",
  "ValuesPerColumns.text" : "Values in columns",
  "AllValues.text": "All Values",
  "DescriptionPredator.text" : "Adaptive Heuristic for Opponent Classification (AdHoc) is an algorithm that allows an agent to create classes of opponents while interacting with them. It was developed for iterated multiagent games (e.g., the iterated Prisoner’s Dilemma) where the environment does not affect the agent’s decisions and it is possible to consider individual and isolated encounters with opponents.",
  "spanish.text": "Spanish",
  "english.text": "English",
  "Languages.text": "Select your default language ",
  "RegistrationSuccessful.text": "107. Registration successful",
  "Registred.text": "User successfully registered! Many thanks, you can now log in",
  "ErrorRegistred.text": "Error! User couldn't be registered. Please, try again",
  "WorkSpaceCreated.text": "New workspace for the model created",
  "CreateWorkSpaceSuccessful.text": "User workspace created successfully",
  "ErrorWorkSpace.text": "Error! User workspace couldn't be created. Please, try again",
  "userName.text": "User Name",
  "FirstName.text": "First name",
  "LastName.text": "Last name",
  "ClusteringChart.text": "Clustering Chart",
  "TitleKmeans.text" : "Choose the properties to create your prediction model. Please note that depending on what you choose you will have a better or worse prediction model",
  "DescriptionKmeans.text" : "Clustering is a form of unsupervised learning that tries to find patterns or groups in the data without using any labels or target values",
  "KmeansSplit.text" : "Split Dataset: ",
  "KmeansSplitTitle.text" : "This parameter is for designing models to be efficient on big data by using a probabilistic splitting methods rather than an exact split. For example, when specifying a 75%/25% split, the algorithm will produce a test/train split with an expected value of 75%/25% rather than exactly 75%/25%",
  "KmeansSeed.text" : "Seed:",
  "KmeansSeedTitle.text" : "Specify the random number generator (RNG) seed. The seed is consistent for each algorithm instance so that you can create models with the same starting conditions in alternative configurations",
  "KmeansInit.text" : "Init:",
  "KmeansInitTitle.text" : "Specify the initialization mode:\n- Random initialization randomly samples the k-specified value of the training data rows as cluster centers.\n- PlusPlus initialization randomly chooses one initial center and weights the random selection of subsequent centers so that points furthest from the first center are more likely to be chosen.\n- Furthest initialization randomly chooses one initial center and then chooses the next center to be the point furthest away in terms of Euclidean distance.\n- User initialization requires the corresponding user_points parameter. Note that the user-specified points dataset must have the same number of columns as the training dataset.",
  "KmeansMaxIteration.text": "Max Iterations:",
  "KmeansMaxIterationTitle.text": "Specify the maximum number of training iterations",
  "KmeansClusters.text": "Clusters:", 
  "KmeansClustersTitle.text": "Specify the number of clusters (groups of data) in a dataset that are similar to one another",
  "KmeansStandarize.text": "Normalize: ",
  "KmeansStandarizeTitle.text": "Enable this option to normalize the numeric columns to have a mean of zero and unit variance. Normalization is highly recommended. If you do not use normalization, the results can include components that are dominated by variables that appear to have larger variances relative to other attributes as a matter of scale, rather than true contribution",
  "KmeansClassColumn.text": "Column class",
  "KmeansClassColumnTitle.text": "Column we want predict",
  "Engagement.text":  "Engagement",
  "DataMatrix.text": "Data Matrix",
  "RegressionChart.text": "Regression Chart",
  "TotalWithinSS.text": "Total within SS",
  "BestModelPrediction.text": "Best cluster predicted",
  "Dispersion.text": "Dispersion of the best cluster displaying the columns used to generate the model",
  "SettingsSave.text": "Save settings",
  "ChargeSettings.text": "Settings loaded",
  "CorrelationMatrixKendall.text": "Correlation matrix - Kendall",
  "CorrelationMatrixPearson.text": "Correlation matrix - Pearson",
  "CorrelationMatrixSpearman.text": "Correlation matrix  - Spearman",
  "CorrelationMatrixColumnsSelectedID.text": "Correlation matrix",
  "Goodbye.text": "Logged out, goodbye!!",
  "GLMFamily.text": "Family:",
  "GLMFamilyTitle.text": "The following are the available families:\n- GAUSSIAN: it must be numeric (Real or Int). (this is the default family).\n- BINOMIAL: it must be categorical with 2 levels/classes or binary (Enum or Int).\n- Poisson: it must be numeric and non-negative (Int).\n- GAMMA: it must be numeric, continuous and positive (Real or Int).\n- TWEEDIE: it must be numeric and continuous (Real) and non-negative.\n- MULTINOMIAL: it can be categorical with more than two levels/classes (Enum).\n- ORDINAL: it must be categorical with at least 3 levels.\n- QUASIBINOMIAL: it must be numeric.\n- NEGATIVE BINOMIAL: it must be numeric and non-negative (Int).",
  "GLMSolver.text": "Solver:",
  "GLMSolverTitle.text": "Specify the solver to use (AUTO, IRLSM, L_BFGS, COORDINATE_DESCENT_NAÏVE, COORDINATE_DESCENT, GRADIENT_DESCENT_LH, or GRADIENT_DESCENT_SQERR):\n- IRLSM is quicker on problems with a small number of predictors and for lambda search with L1 penalty.\n- L_BFGS scales better for datasets with many columns.\n- COORDINATE_DESCENT is like IRLSM with the covariance updates version of cyclical coordinate descent in the innermost loop.\n- COORDINATE_DESCENT_NAÏVE is like IRLSM with the naïve updates version of cyclical coordinate descent in the innermost loop.\n- GRADIENT_DESCENT_LH and GRADIENT_DESCENT_SQERR can only be used with the ORDINAL family.",
  "GLMLambda.text": "Lambda:",
  "GLMLambdaTitle.text": "Specify the regularization strength",
  "GLMMissing.text" : "Missing values handling:",
  "GLMMissingTitle.text" : "Specify how to handle missing values",
  "GLMTweedie.text" : "Tweedie:",
  "GLMTweedieTitle.text" : "(Only applicable if Tweedie is specified under the Family parameter) Specify the Tweedie variance power",
  "GLMLink.text" : "Link:",
  "GLMLinkTitle.text" : "The following are the links supported classified by families:\n- GAUSSIAN: Identity, Log, and Inverse.\n- BINOMIAL: then Logit.\n- POISSON: then Log and Identity.\n- GAMMA: then Inverse, Log, and Identity.\n- TWEEDIE: then only Tweedie.\n- MULTINOMIAL: then only Family_Default (this defaults to multinomial)\n- QUASIBINOMIAL: then only Logit.\n- ORDINAL: then Ologit, Oprobit, and Ologlog. (note that only Ologit is available for Ordinal regression)\n- NEGATIVE BINOMIAL: then Log and Identity.",
  "GLMIgnore.text": "Ignore:",
  "GLMIgnoreTitle.text": "Specify whether to enable lambda search, starting with lambda max (the smallest lambda that drives all coefficients to zero)",
  "GLMAlpha.text": "Alpha:",
  "GLMAlphaTitle.text": "Specify the maximum number of training iterations.",
  "GLMLambdaSearch.text": "Lambda Search:",
  "GLMLambdaSearchTitle.text": "Specify the maximum number of training iterations",
  "GLMNfolds.text": "Folds:",
  "GLMNfoldsTitle.text": "Specify the number of folds for cross-validation.",
  "GLMKeeppredSearch.text": "Cross validation:",
  "GLMKeeppredTitle.text": "Specify whether or not to keep the cross-validation predictions" ,
  "GLMColumnDepentVar.text": "Dependant variable:",
  "GLMColumnDepentVarTitle.text": "Specify the column to use as the dependent variable:\n- For a regression model, this column must be numeric (Real or Int).\n- For a classification model, this column must be categorical (Enum or String). If the family is Binomial, the dataset cannot contain more than two levels.",
  "PredictionGLM.text": "Prediction",
  "DispersionGLM.text": "Dispersion of the columns used to generate the model",
  "Main1.text": "Machine learning (ML) can help you by using historical data to make better business decisions. ML algorithms discover patterns in data and construct mathematical models using these discoveries. Then you can use the models to make predictions on future data.",
  "Main2.text": "Use machine learning for the following situations:",
  "Main3.text": "- You cannot code the rules: Many human tasks (such as recognizing whether an email is spam or not) cannot be adequately solved using a simple (deterministic), rule-based solution. A large number of factors could influence the answer. When rules depend on too many factors and many of these rules overlap or need to be tuned very finely, it soon becomes difficult for a human to accurately code the rules. You can use ML to effectively solve this problem.",
  "Main4.text": "- You cannot scale: You might be able to manually recognize a few hundred emails and decide whether they are spam or not. However, this task becomes tedious for millions of emails. ML solutions are effective at handling large-scale problems.",
  "Main5.text": "What is a datasource?",
  "Main6.text": "Machine learning requires that the correct set of data be applied to a learning process to ensure that the accuracy of the generated is optimized; this application uses historic data stored in CSV format files.",
  "Main7.text": "What is the training?",
  "Main8.text": "The process of training an ML model involves providing the ML algorithm (that is, the learning algorithm) with training data to learn from. The term ML model refers to the model artifact that is created by the training process.",
  "Main9.text": "The training data must contain the correct answer, which is known as a target or target attribute. The learning algorithm finds patterns in the training data that map the input data attributes to the target (the answer that you want to predict), and it outputs an ML model that captures these patterns.",
  "Main10.text": "What is a model?",
  "Main11.text": "A machine learning model is the output generated when you train your machine learning algorithm with historic data. After training, the model will be provided with an input. For example, an Anomaly detection model can consume a data streaming to identify anomalies in real time based on the data that trained the model",
  "PredatorRequirementsTitol.text": "Requirements",
  "PredatorRequirements.text": "- Upload the files in groups of 3 files for the same workflow, namely left profile (_1.csv), right profile (_2.csv) and GPS (_gps.csv).",
  "PredatorRequirements2.text": "- The files must have this filename format: `200117-002740_depot_gps.csv, 200117-002740_depot_1.csv, 200117-002740 _depot_2.csv`, without spaces, and with the first 13 values of the filename in this format, e.g. `200117-002740_` and the last value of the filename `_1`,` _2` and `_gps`.",
  "PredatorRequirementsLeft.text": "Left profile",
  "PredatorRequirementsRight.text": "Right profile",
  "PredatorRequirementsGPS.text": "GPS profile",
  "PredatorRequirementsDataset.text": "Dataset file CSV or JSON",
 "PredatorRequirementsSensor.text": "Name of sensor",
  "PredatorRequirementsNumberRow.text": "Rows to generate encounters models",
  "PredatorServerKafka.text": "Kafka Server Address",
  "PredatorSubmit.text": "Submit",
  "GLMRegression.text": "GLM Regression",
  "GLMDescription.text" :  "Generalized Linear Models (GLM) estimate regression models for outcomes following exponential distributions. In addition to the Gaussian (i.e. normal) distribution, these include Poisson, Binomial, and Gamma distributions. Each serves a different purpose and depending on distribution and link function selected, it can be used either for prediction or classification",
  "ModelDeleted.text": "successfully deleted",
  "ModelDeletedError.text": "couldn't be deleted. Please, try again",
  "DeleteModel.text" : "Delete Model",
  "SaveModel.text": "Your model has been saved in your workspace",
  "WaitingPythonScripts.text" : "Awaiting data, please be patient",
  "EndingPythonScripts.text": "Finishing",
  "BuildingtraindataPythonScripts.text": "Building training data",
  "BuildingtestdataPythonScripts.text": "Building testing data",
  "StartingPythonScripts.text": "Initialising",
  "BuildingGraphicsPythonScripts.text" : "Building charts",
  "SavingGraphicsPythonScripts.text": "Saving charts",
  "BuildingCorrelationsPythonScripts.text":"Building correlations",
  "PrintingCoorelationAnalysisPythonScripts.text" : "Printing correlation analysis",
  "SavingCoorelationsPythonScripts.text": "Saving correlations",
  "BuidingOjectCreateModelPythonScripts.text": "Building objects to create model",
  "BuidingModelPythonScripts.text": "Building models",
  "BuidingMSEPythonScripts.text":"Building reconstructed error (MSE)",
  "BuidingModelsAnomaliesPythonScripts.text":"Building models anomalies",
  "RecoveringModelPythonScripts.text":"Retrieving model",
  "BuidingPredictionsPythonScripts.text": "Building predictions",
  "OrderingModelPredictionsAnomaliesPythonScripts.text": "Sorting model, predictions and anomalies",
  "SortingcoorelationsPythonScripts.text": "Sorting correlations",
  "BuildingDatasetPythonScripts.text": "Building dataset",
  "BuildingDataChooseenUsersPythonScripts.text": "Building data selected by the users",
  "BuildingDataRegressionPythonScripts.text" : "Building data regression",
  "DrawPlotsPythonScripts.text":"Draw plots",
  "ErrorFailsPythonScripts.text": "Generic error. Sorry, please try again, thanks",
  "ErrorDataInputPythonScripts.text" : "Error in data input. Sorry, please try again, thanks",
  "ErrorAlgorithmPythonScripts.text" : "Error in algorithm. Sorry, please try again, thanks",
  "Recognition.text" : "Image recognition algorithm",
  "CNNText.text" : "Convolutional Neural Networks",
  "CNNs.text" : "(CNNs)",
  "RNNText.text" : "Recurrent Neural Networks",
  "RNNs.text" : "(RNNs)",
  "RBMText.text" : "Restricted Boltzmann Machines",
  "RBMs.text" : "(RBMs)",
  "AutoencodersText.text" : "Autoencoders",
  "DescriptionCNN.text" : "Convolutional Neural Networks a special type of neural network that roughly imitates human vision.",
  "DescriptionRNN.text" : "A recurrent neural network is a type of artificial neural network commonly used in speech recognition and natural language processing. Recurrent neural networks recognize data's sequential characteristics and use patterns to predict the next likely scenario.",
  "DescriptionRBN.text" : "Restricted Boltzma They can be thought of as feed forward neural networks with feedback loops or back propagation through time. Basically, they are MLP(Multi-Layer Perceptron) with an additional Time variable Machines",
  "DescriptionAutoencoder.text" : "Autoencoders are neural networks that aim to generate new data by first compressing the input into a latent variable space and then reconstructing the output based on the acquired information.",
  "PicturesTraining.text": "Training Images",
  "FirstLayerConvolution.text": "First image convolution layer to train the model in the neural network.",
  "SecondLayerConvolution.text": "Last image set convolution layer of the neural network model",
  "SingleTestCNN.text": "Simple test for a random image of the validation images with the newly constructed model",
  "GroupTestCNN.text": "Test with a set of saved validation images to check the validity of the model.",
  "EntropyCNN.text": "Cross Entropy Loss and Classification Accuracy ",
  "RedTestCNN.text": "Text in red badly identified image.",
  "TitleTestModelCNN.text": "Validation of the Model: ",
  "DragANDDropCNN.text": "Drag and drop file here and test your model",
  "BrowseFilesCNN.text": "Browse for file",
  "NumberImages.text": "Number of classes or type images: (max.10)",
  "ListImagesMenu.text" : "List of Images",
  "LoadImagesUI.text": "Upload Images",
  "GenerareClass.text": "Generate classes",
  "BatchNormalization.text": "Batch Normalization",
  "layerConvolution.text": "Convolution number the layers in neuronal network.",
  "Dropout.text": "% dropout the layers: ",
  "Dense.text": "Density neuronal in layer: ",
  "layer.text": "Number the layers in neuronal network.",
  "epochs.text": "Number the epochs in neuronal network",
  "numberNeuros.text": "Number the neurones in neuronal network.",
  "activation.text": "Activation",
  "TiltleAction.text": "An activation function in a neural network defines how the weighted sum of the input is \n transformed into an output from a node or nodes in a layer of the network",
  "relu.text": "Rectified Linear Activation (ReLU)",
  "sigmoid.text": "Logistic (Sigmoid)",
  "tanh.text": "Hyperbolic Tangent (Tanh)",
  "softmax.text": "Normalised exponential function (softmax)",
  "InserClasses1.text": " Minimum images per this model are ",
  "InserClasses2.text": ", current images: ",
  "Accuracy.text": " Accuracy ",
  "Loss.text": " Loss ",
  "InformationCNN.text": "If you want to test the model look for it in the 'Mymodels' section.",
  "cnn.text": "Models CNN",
  "glm.text": "Models GLM",
  "id.text": "Models per ID",
  "kmeans.text": "Models Clustering",
  "columns.text": "Models per Columns",
  "all.text": "All models",
  "SearchPerModel.text": " Filter by model.",
  "Nlayer.text": " layers",
  "SearchPerName.text": " Search per Name. ",
  "NotFound.text": " Not Found ",
  "featurewiseCenter.text" : "Feature wise center",
  "classMode.text": "Class mode",
  "classModeTitle.text": "Set 'binary' if you have only two classes to predict, if not set to 'categorical', in case if you're developing an Autoencoder system, both input and the output would probably be the same image, for this case set to 'input'",
  "colorMode.text": "Color Mode",
  "colorModeTitle.text": "if the image is either black and white or grayscale set 'grayscale' or if the image has three color channels, set 'rgb'.",
  "shuffle.text": "Shuffle",
  "shuffleTitle.text": "Set True if you want to shuffle the order of the image that is being yielded, else set False.",
  "seed.text": "Seed",
  "seedTitle.text": "Random seed for applying random image augmentation and shuffling the order of the image.",
  "batchSize.text": "Batch Size",
  "batchSizeTitle.text": "Number of images to be yielded from the generator per batch",
  "imgHeight.text": "Resize per Height",
  "imgHeightTitle.text": "is the size of your input images, every image will be resized to this size, per height",
  "imgWidth.text": "Resize per Width",
  "imgWidthTitle.text": "is the size of your input images, every image will be resized to this size, per width",
  "learningRate.text": "Learning Rate",
  "learningRateTitle.text": "The learning rate. Defaults to 0.01. ",
  "momentum.text": "Momentum",
  "momentumTitle.text": "Float hyperparameter >= 0 that accelerates gradient descent in the relevant direction and dampens oscillations. Defaults to 0, i.e., vanilla gradient descent.  ",
  "lossCompile.text": "Loss function",
  "lossCompileTitle.text": "Maybe be a string (name of loss function), or a tf.keras.losses.Loss instance. See tf.keras.losses. A loss function is any callable with the signature loss = fn(y_true, y_pred), where y_true are the ground truth values, and y_pred are the model's predictions. y_true should have shape (batch_size, d0, .. dN) (except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape (batch_size, d0, .. dN-1)). y_pred should have shape (batch_size, d0, .. dN). The loss function should return a float tensor. If a custom Loss instance is used and reduction is set to None, return value has shape (batch_size, d0, .. dN-1) i.e. per-sample or per-timestep loss values; otherwise, it is a scalar. If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses, unless loss_weights is specified. ",
  "sparse_categorical_crossentropyTitle.text": "Computes the crossentropy loss between the labels and predictions.",
  "binary_crossentropyTitle.text": "Computes the cross-entropy loss between true labels and predicted labels.",
  "categorical_crossentropyTitle.text": "Computes the crossentropy loss between the labels and predictions.",
  "categorical_hingeTitle.text": "Computes the categorical hinge loss between y_true and y_pred.",
  "cosine_similarityTitle.text": "Computes the cosine similarity between labels and predictions.",
  "hingeTitle.text": "Computes the hinge loss between y_true and y_pred.",
  "huberTitle.text": "Computes the Huber loss between y_true and y_pred.",
  "kldivergenceTitle.text": "Computes Kullback-Leibler divergence loss between y_true and y_pred.",
  "log_coshTitle.text": "Computes the logarithm of the hyperbolic cosine of the prediction error.",
  "lossTitle.text": "Loss base class.",
  "mean_absolute_errorTitle.text": "Computes the mean of absolute difference between labels and predictions.",
  "mean_absolute_percentage_errorTitle.text": "Computes the mean absolute percentage error between y_true and y_pred.",
  "mean_squared_errorTitle.text": "Computes the mean of squares of errors between labels and predictions.",
  "mean_squared_logarithmic_errorTitle.text": "Computes the mean squared logarithmic error between y_true and y_pred.",
  "poissonTitle.text": "Computes the Poisson loss between y_true and y_pred.",
  "reductionTitle.text": "Types of loss reduction.",
  "squaredhingeTitle.text": "Computes the squared hinge loss between y_true and y_pred.",
  "kernelInitializer.text": "Kernel Initializer",
  "kernelInitializerTitle.text": "Initializers define the way to set the initial random weights of Keras layers. The keyword arguments used for passing initializers to layers depends on the layer. .",
  "ConstantTitle.text": "Initializer that generates tensors with constant values.",
  "GlorotNormalTitle.text": "The Glorot normal initializer, also called Xavier normal initializer.",
  "GlorotUniformTitle.text": "The Glorot uniform initializer, also called Xavier uniform initializer.",
  "HeNormalTitle.text": "He normal initializer.",
  "HeUniformTitle.text": "He uniform variance scaling initializer.",
  "IdentityTitle.text": "Initializer that generates the identity matrix.",
  "InitializerTitle.text": "Initializer base : all Keras initializers inherit from this .",
  "LecunNormalTitle.text": "Lecun normal initializer.",
  "LecunUniformTitle.text": "Lecun uniform initializer.",
  "OnesTitle.text": "Initializer that generates tensors initialized to 1.",
  "OrthogonalTitle.text": "Initializer that generates an orthogonal matrix.",
  "RandomNormalTitle.text": "Initializer that generates tensors with a normal distribution.",
  "RandomUniformTitle.text": "Initializer that generates tensors with a uniform distribution.",
  "TruncatedNormalTitle.text":" Initializer that generates a truncated normal distribution.",
  "VarianceScalingTitle.text":" Initializer capable of adapting its scale to the shape of weights tensors.",
  "ZerosTitle.text":" Initializer that generates tensors initialized to 0.",
  "typeImages.text": "Type images",
  "ClusteringH2O.text": "Clustering by H2O Kmeans Algorithm",
  "ClusteringTensor.text": "Clustering by TensorFlow Algorithm",
  "ClusteringWard.text": "Clustering by Ward Algorithm",
  "ClusteringWardAlgorithm.text": "Clustering algorithm Ward",
  "ClusteringBirchAlgorithm.text": "Clustering algorithm Birch",
  "ClusteringBirch.text": "Clustering by Birch Algorithm",
  "ClusteringSklearn.text": "Clustering by Sklearn Kmeans Algorithm",
  "ClusteringSklearnAlgorithm.text": "Clustering algorithm Sklearn",
  "SklearnClusteringRandomState.text": "Random State: ",
  "SklearnClusteringRandomStateTitle.text": "Determines random number generation for centroid initialization. Use an int to make the randomness deterministic.",
  "SklearnnInit.text": "Number init: ",
  "SklearnnInitTitle.text": "Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia.",
  "WaterFall.Text" : "This plot helps us visualise the SHAP values for each of the features. These tell us how much each of the features have increased or decreased the predicted number of columns for this specific column.",
  "MeanBar.Text": "Another way to aggregate the values is using a mean SHAP plot. For each feature, we calculate the mean of the absolute SHAP values across all observations. We take the absolute values as we do not want positive and negative values to offset each other. There is one bar for each feature and we can see that shell weight had the largest mean SHAP out of all the features.",
  "SummaryPlotBarViolin.Text": "The SHAP summary plot can show the positive and negative relationships of the predictors with the target variable, in violin plot",
  "SummaryPlotBarDot.Text": "This is a plot of all the SHAP values. The values are grouped by the features on the y-axis. For each group, the colour of the points is determined by the value of the same feature (i.e. higher feature values are redder). The features are ordered by the mean SHAP values. ",
  "DecisionPlot.Text": "We can already see some trends. For example, some of the lines seem to zigzag at the top of the graph. For these observations, the weight of the some columns increases the prediction (i.e. positive SHAP) and the shell weight and the whole weight decrease the prediction (i.e. negative SHAP). In other words, these features have opposite effects on the prediction",
  "HeatMap.Text" : "A heatmap visualizes the magnitude of data in two dimensions. The variation in color can give visual cues about how the data are clustered. You can use the hot-to-cold color scheme to show the relationships. The variable importance is shown in descending order in the Y-axis, with the bars resembling the bars in the bar chart. The f(x) curve on the top is the model predictions for the instances. The SHAP first runs a hierarchical clustering on the instances to cluster them, then orders the instances on the X-axis. The center of the 2D heatmap is the base_value, which is the mean prediction for all instances.",
  "ShapExplanation.Text" : "Explanation of the model built by the SHAP method",
  "DataPlotEx.Text": "Original data set",
  "Weight.Text": "We can focus on these relationships using the scatter plots. Here we can clearly see how the relationship between SHAP values and feature values is positive and negative respectfully.",
  "Legend.Text": "Some data can be divided into two or more groups according to predictors. This provides more information about the heterogeneity of the data. You can use cohorts(N) to divide the population into N cohorts.",
  "Observation.Text": "Every SHAP value.\n The x-axis represents the model's output. In this case, the units are log odds.\n The plot is centered on the x-axis at explainer.expected_value. All SHAP values are relative to the model's expected value like a linear model's effects are relative to the intercept.\n The y-axis lists the model's features. By default, the features are ordered by descending importance. The importance is calculated over the observations plotted. This is usually different than the importance ordering for the entire dataset. In addition to feature importance ordering, the decision plot also supports hierarchical cluster feature ordering and user-defined feature ordering.\n Each observation's prediction is represented by a colored line. At the top of the plot, each line strikes the x-axis at its corresponding observation's predicted value. This value determines the color of the line on a spectrum.\n Moving from the bottom of the plot to the top, SHAP values for each feature are added to the model's base value. This shows how each feature contributes to the overall prediction.\n At the bottom of the plot, the observations converge at explainer.expected_value.",
  "DecisionPlotAll.Text": "All SHAP values, but link logic.\n The x-axis represents the model's output. In this case, the units are log odds.\n The plot is centered on the x-axis at explainer.expected_value. All SHAP values are relative to the model's expected value like a linear model's effects are relative to the intercept.\n The y-axis lists the model's features. By default, the features are ordered by descending importance. The importance is calculated over the observations plotted. This is usually different than the importance ordering for the entire dataset. In addition to feature importance ordering, the decision plot also supports hierarchical cluster feature ordering and user-defined feature ordering.\n Each observation's prediction is represented by a colored line. At the top of the plot, each line strikes the x-axis at its corresponding observation's predicted value. This value determines the color of the line on a spectrum.\n Moving from the bottom of the plot to the top, SHAP values for each feature are added to the model's base value. This shows how each feature contributes to the overall prediction.\n At the bottom of the plot, the observations converge at explainer.expected_value.",
  "DecisionPlotAllN.Text": "All SHAP values, but link normal.\n The x-axis represents the model's output. In this case, the units are log odds.\n The plot is centered on the x-axis at explainer.expected_value. All SHAP values are relative to the model's expected value like a linear model's effects are relative to the intercept.\n The y-axis lists the model's features. By default, the features are ordered by descending importance. The importance is calculated over the observations plotted. This is usually different than the importance ordering for the entire dataset. In addition to feature importance ordering, the decision plot also supports hierarchical cluster feature ordering and user-defined feature ordering.\n Each observation's prediction is represented by a colored line. At the top of the plot, each line strikes the x-axis at its corresponding observation's predicted value. This value determines the color of the line on a spectrum.\n Moving from the bottom of the plot to the top, SHAP values for each feature are added to the model's base value. This shows how each feature contributes to the overall prediction.\n At the bottom of the plot, the observations converge at explainer.expected_value.",
  "BarForce.Text": "Another way to visualise the SHAP values is by means of a force chart. These give us practically the same information as a waterfall chart. Here, we can see that we start with the same base value. You can also see how each characteristic increases/decreases the predicted value to give us the final prediction.",
  "Multiclass.Text": "You can use the summary plot to show the variable importance by class",
  "ShapMulticlass.Text": "You may have built a multiclass model that classifies instances into several classes. \n The output of a multiclass model is a probability matrix for the classes. If we have three classes, so the outputs are the probabilities for the three classes, which add up to 1.0. The probabilities are displayed, as shown in the class columns.\n The .predict() function displays the predicted classification, as shown in the 'Pred' column. ",
  "SklearnClusteringTBirchhresholdTitle.Text": "The radius of the subcluster obtained by merging a new sample and the closest subcluster should be lesser than the threshold.\n Otherwise a new subcluster is started. Setting this value to be very low promotes splitting and vice-versa.",
  "SklearnClusteringTBirchhresholdState.Text": "Threshold: ",
  "SklearnBirchBranchingFactorTitle.Text": "Maximum number of CF subclusters in each node. If a new samples enters such that the number of subclusters exceed the branching_factor then that node is split into two nodes with the subclusters redistributed in each. \n  The parent subcluster of that node is removed and two new subclusters are added as parents of the 2 split nodes.",
  "SklearnBirchBranchingFactor.Text": "Branching Factor: ",
  "SklearnBirchComputeLabelsTitle.Text": "Whether or not to compute labels for each fit.",
  "SklearnBirchComputeLabels.Text": "Compute Labels: ",
  "SklearnClusteringTWardaffinityTitle.Text": "Metric used to compute the linkage.",
  "SklearnClusteringTWardaffinityState.Text": "Affinity: ",
  "SklearnWardcomputeFullTreeTitle.Text": "Stop early the construction of the tree at n_clusters. This is useful to decrease computation time if the number of clusters is not small compared to the number of samples. \n This option is useful only when specifying a connectivity matrix. Note also that when varying the number of clusters and using caching, it may be advantageous to compute the full tree. \n  It must be True if distance_threshold is not None. By default compute_full_tree is “auto”, which is equivalent to True when distance_threshold is not None \n  or that n_clusters is inferior to the maximum between 100 or 0.02 * n_samples. Otherwise, “auto” is equivalent to False.",
  "SklearnWardcomputeFullTree.Text": "Compute Full Tree: ",
  "SklearnWardcomputeDistancesTitle.Text": "Computes distances between clusters even if distance_threshold is not used. This can be used to make dendrogram visualization, but introduces a computational and memory overhead.",
  "SklearnWardcomputeDistances.Text": "Compute Distances: ",
  "CleanDataPythonScripts.Text": "Clean data ",
  "GenerateShapPlotsDataPythonScripts.Text": "Generate Shap Plots & Data ",
  "BuildingKMEANSAlgorithmPythonScripts.Text": "Building K-MEANS Clustering SSE ",
  "ClustersSilhouettePythonScripts.Text": "Building Number of Clusters vs. Silhouette Score ",
  "BuildingModelBirchPythonScripts.Text": "Building model Birch ",
  "BuildingRelativeImportanceAttributesPythonScripts.Text": "Building Relative Importance of Attributes ",
  "BuildingRelativeImportanceHeatmapPythonScripts.Text": "Building Relative Importance Heatmap ",
  "SaveModelJSONPythonScripts.Text": "Save model & JSON file ",
  "CompletingFinalOperationsPythonScripts.Text": "Completing final operations ",
  "BuildingModelSingle.Text": "Building model Single ",
  "BuildingModelComplete.Text": "Building model Complete ",
  "BuildingModelWard.Text": "Building model Ward ",
  "BuildingModelAverage.Text": "Building model Average ",
  "SeeDistorsions.Text": "SEE and Distorsions by Cluster Attributes",
  "PredictionLabel.Text": "Cluster Prediction",
  "TrueLabel.Text": "Original Data",
  "BIRCHBarStandardizedAttributes.Text": "BIRCH: Bar Plot of Standardized Attributes",
  "BIRCHSnakeStandardizedAttributes.Text": "BIRCH: Snake Plot of Standardized Attributes",
  "BIRCHRelativeImportanceAttributes.Text": "BIRCH: Relative Importance of Attributes" ,
  "AllColumnsForTrainModel.Text": "All Columns For Train Model",
  "DeleteAllColumns.text": "Delete All Columns",
  "ClustingSingleLink.Text": "Clusting by Single Link",
  "ClustingCompleteLink.Text": "Clusting by Complete Link",
  "ClustingWardLink.Text": "Clusting by Ward Link",
  "ClustingAverageLink.Text": "Clusting by Average Link",
  "SnakePlot.Text":"Snake Plot",
  "BarPlot.Text": "Bar Plot",
  "LinkName.Text": "Linkage:",
  "linkTitle.Text": "Which linkage criterion to use. The linkage criterion determines which distance to use between sets of observation. \n The algorithm will merge the pairs of cluster that minimize this criterion. \n 'ward' minimizes the variance of the clusters being merged. \n 'average' uses the average of the distances of each observation of the two sets. \n 'complete' or ‘maximum’ linkage uses the maximum distances between all observations of the two sets. \n 'single' uses the minimum of the distances between all observations of the two sets.",
  "ValuesNan.Text": "NULL & NAN values",
  "ValuesNanTitle.Text": "This part es just for how you want handle NULL & NAN values of the datasets: \n - Drop: Delete rows with values with NULL & NaN. \n - With0: Fills NULL & NaN values with zeros. \n - Fill: Propagate non-null values forward or backward. \n - Quartile min: Fill the NULL & NaN values in the column with the lowest quartile of the column values. \n - Quartile 25%:  Fill the NULL & NaN values in the column with the 25% quartile of the column values. \n - Quartile 75%:  Fill the NULL & NaN values in the column with the 75% quartile of the column values. \n - Quartile Max:  Fill the NULL & NaN values in the column with the maximum quartile of the column values. \n - Median:  Fill the NULL & NaN values in the column with the median of the column values. \n - Mean:  Fill the NULL & NaN values in the column with the mean of the column values. \n - Mode: Replace with the most repeated value in the column. ",
  "Authorization.Text": "Sorry but you do not have permission to use this algorithm, please contact the administrator." ,
  "Authentication.Text": "Sorry, but you are not authenticated in the system, please contact the administrator.",
  "DescriptionGAN.text" : "Generative adversarial networks (GANs) are algorithmic architectures that use two neural networks, pitting one against the other (thus the “adversarial”) in order to generate new, synthetic instances of data that can pass for real data. They are used widely in image generation, video generation and voice generation.",
  "GANText.Text" : "GAN X Ray" ,
  "GANs.Text" : "(GAN)",
  "AdvancedModelTitle.Text": "Model has been trained on primary data only or primary + secondary data",
  "SklearnBirchAdvancedModel.Text" : "Advanced models",
  "Descriptionward.text" : "The Ward clustering algorithm in scikit-learn is a technique that groups data hierarchically to minimise variability within each group. It is useful for finding similar groups of data and is based on gradually merging clusters close to each other. This approach is commonly used in data analysis and segmentation.",
  "DescriptionBirch.text" : "BIRCH is a clustering algorithm in scikit-learn that stands out for its ability to handle large datasets by constructing a BIRCH tree, making it efficient and scalable for clustering applications.",
  "DescriptionskClust.text" : "K-Means is a clustering algorithm that groups data into K clusters, where K is a pre-defined number. The main goal is to assign each data point to the nearest cluster according to its distance from the cluster centroids. Scikit-learn provides a flexible and easy-to-use implementation of K-Means that allows you to specify the number of clusters and adjust the model to your data."




  }